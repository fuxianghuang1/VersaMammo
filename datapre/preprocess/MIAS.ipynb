{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIAS-breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mdb001 for Train set\n",
      "Processed mdb003 for Train set\n",
      "Processed mdb005 for Train set\n",
      "Processed mdb007 for Train set\n",
      "Processed mdb009 for Train set\n",
      "Processed mdb011 for Train set\n",
      "Processed mdb014 for Train set\n",
      "Processed mdb015 for Train set\n",
      "Processed mdb018 for Train set\n",
      "Processed mdb019 for Train set\n",
      "Processed mdb020 for Train set\n",
      "Processed mdb021 for Train set\n",
      "Processed mdb022 for Train set\n",
      "Processed mdb023 for Train set\n",
      "Processed mdb024 for Train set\n",
      "Processed mdb025 for Train set\n",
      "Processed mdb027 for Train set\n",
      "Processed mdb028 for Train set\n",
      "Processed mdb029 for Train set\n",
      "Processed mdb030 for Train set\n",
      "Processed mdb031 for Train set\n",
      "Processed mdb032 for Train set\n",
      "Processed mdb033 for Train set\n",
      "Processed mdb035 for Train set\n",
      "Processed mdb036 for Train set\n",
      "Processed mdb037 for Train set\n",
      "Processed mdb038 for Train set\n",
      "Processed mdb040 for Train set\n",
      "Processed mdb041 for Train set\n",
      "Processed mdb042 for Train set\n",
      "Processed mdb044 for Train set\n",
      "Processed mdb045 for Train set\n",
      "Processed mdb048 for Train set\n",
      "Processed mdb049 for Train set\n",
      "Processed mdb050 for Train set\n",
      "Processed mdb051 for Train set\n",
      "Processed mdb052 for Train set\n",
      "Processed mdb053 for Train set\n",
      "Processed mdb054 for Train set\n",
      "Processed mdb055 for Train set\n",
      "Processed mdb057 for Train set\n",
      "Processed mdb059 for Train set\n",
      "Processed mdb060 for Train set\n",
      "Processed mdb062 for Train set\n",
      "Processed mdb063 for Train set\n",
      "Processed mdb065 for Train set\n",
      "Processed mdb066 for Train set\n",
      "Processed mdb067 for Train set\n",
      "Processed mdb068 for Train set\n",
      "Processed mdb069 for Train set\n",
      "Processed mdb070 for Train set\n",
      "Processed mdb072 for Train set\n",
      "Processed mdb073 for Train set\n",
      "Processed mdb074 for Train set\n",
      "Processed mdb075 for Train set\n",
      "Processed mdb080 for Train set\n",
      "Processed mdb081 for Train set\n",
      "Processed mdb084 for Train set\n",
      "Processed mdb086 for Train set\n",
      "Processed mdb087 for Train set\n",
      "Processed mdb088 for Train set\n",
      "Processed mdb089 for Train set\n",
      "Processed mdb090 for Train set\n",
      "Processed mdb092 for Train set\n",
      "Processed mdb093 for Train set\n",
      "Processed mdb096 for Train set\n",
      "Processed mdb097 for Train set\n",
      "Processed mdb098 for Train set\n",
      "Processed mdb099 for Train set\n",
      "Processed mdb100 for Train set\n",
      "Processed mdb101 for Train set\n",
      "Processed mdb103 for Train set\n",
      "Processed mdb104 for Train set\n",
      "Processed mdb106 for Train set\n",
      "Processed mdb107 for Train set\n",
      "Processed mdb108 for Train set\n",
      "Processed mdb110 for Train set\n",
      "Processed mdb111 for Train set\n",
      "Processed mdb112 for Train set\n",
      "Processed mdb113 for Train set\n",
      "Processed mdb118 for Train set\n",
      "Processed mdb121 for Train set\n",
      "Processed mdb122 for Train set\n",
      "Processed mdb123 for Train set\n",
      "Processed mdb124 for Train set\n",
      "Processed mdb125 for Train set\n",
      "Processed mdb126 for Train set\n",
      "Processed mdb128 for Train set\n",
      "Processed mdb129 for Train set\n",
      "Processed mdb130 for Train set\n",
      "Processed mdb131 for Train set\n",
      "Processed mdb132 for Train set\n",
      "Processed mdb134 for Train set\n",
      "Processed mdb136 for Train set\n",
      "Processed mdb137 for Train set\n",
      "Processed mdb138 for Train set\n",
      "Processed mdb139 for Train set\n",
      "Processed mdb140 for Train set\n",
      "Processed mdb141 for Train set\n",
      "Processed mdb142 for Train set\n",
      "Processed mdb143 for Train set\n",
      "Processed mdb145 for Train set\n",
      "Processed mdb146 for Train set\n",
      "Processed mdb148 for Train set\n",
      "Processed mdb149 for Train set\n",
      "Processed mdb151 for Train set\n",
      "Processed mdb152 for Train set\n",
      "Processed mdb153 for Train set\n",
      "Processed mdb154 for Train set\n",
      "Processed mdb155 for Train set\n",
      "Processed mdb156 for Train set\n",
      "Processed mdb158 for Train set\n",
      "Processed mdb160 for Train set\n",
      "Processed mdb161 for Train set\n",
      "Processed mdb162 for Train set\n",
      "Processed mdb163 for Train set\n",
      "Processed mdb164 for Train set\n",
      "Processed mdb166 for Train set\n",
      "Processed mdb167 for Train set\n",
      "Processed mdb169 for Train set\n",
      "Processed mdb170 for Train set\n",
      "Processed mdb171 for Train set\n",
      "Processed mdb172 for Train set\n",
      "Processed mdb175 for Train set\n",
      "Processed mdb176 for Train set\n",
      "Processed mdb177 for Train set\n",
      "Processed mdb178 for Train set\n",
      "Processed mdb179 for Train set\n",
      "Processed mdb181 for Train set\n",
      "Processed mdb182 for Train set\n",
      "Processed mdb183 for Train set\n",
      "Processed mdb184 for Train set\n",
      "Processed mdb185 for Train set\n",
      "Processed mdb190 for Train set\n",
      "Processed mdb191 for Train set\n",
      "Processed mdb193 for Train set\n",
      "Processed mdb194 for Train set\n",
      "Processed mdb195 for Train set\n",
      "Processed mdb200 for Train set\n",
      "Processed mdb201 for Train set\n",
      "Processed mdb203 for Train set\n",
      "Processed mdb204 for Train set\n",
      "Processed mdb206 for Train set\n",
      "Processed mdb207 for Train set\n",
      "Processed mdb208 for Train set\n",
      "Processed mdb211 for Train set\n",
      "Processed mdb212 for Train set\n",
      "Processed mdb213 for Train set\n",
      "Processed mdb214 for Train set\n",
      "Processed mdb215 for Train set\n",
      "Processed mdb216 for Train set\n",
      "Processed mdb218 for Train set\n",
      "Processed mdb219 for Train set\n",
      "Processed mdb220 for Train set\n",
      "Processed mdb221 for Train set\n",
      "Processed mdb222 for Train set\n",
      "Processed mdb223 for Train set\n",
      "Processed mdb224 for Train set\n",
      "Processed mdb225 for Train set\n",
      "Processed mdb229 for Train set\n",
      "Processed mdb232 for Train set\n",
      "Processed mdb233 for Train set\n",
      "Processed mdb234 for Train set\n",
      "Processed mdb235 for Train set\n",
      "Processed mdb236 for Train set\n",
      "Processed mdb237 for Train set\n",
      "Processed mdb238 for Train set\n",
      "Processed mdb239 for Train set\n",
      "Processed mdb240 for Train set\n",
      "Processed mdb241 for Train set\n",
      "Processed mdb242 for Train set\n",
      "Processed mdb243 for Train set\n",
      "Processed mdb246 for Train set\n",
      "Processed mdb247 for Train set\n",
      "Processed mdb248 for Train set\n",
      "Processed mdb250 for Train set\n",
      "Processed mdb251 for Train set\n",
      "Processed mdb252 for Train set\n",
      "Processed mdb253 for Train set\n",
      "Processed mdb254 for Train set\n",
      "Processed mdb255 for Train set\n",
      "Processed mdb258 for Train set\n",
      "Processed mdb259 for Train set\n",
      "Processed mdb260 for Train set\n",
      "Processed mdb261 for Train set\n",
      "Processed mdb263 for Train set\n",
      "Processed mdb264 for Train set\n",
      "Processed mdb265 for Train set\n",
      "Processed mdb266 for Train set\n",
      "Processed mdb267 for Train set\n",
      "Processed mdb268 for Train set\n",
      "Processed mdb273 for Train set\n",
      "Processed mdb274 for Train set\n",
      "Processed mdb275 for Train set\n",
      "Processed mdb276 for Train set\n",
      "Processed mdb277 for Train set\n",
      "Processed mdb278 for Train set\n",
      "Processed mdb279 for Train set\n",
      "Processed mdb280 for Train set\n",
      "Processed mdb281 for Train set\n",
      "Processed mdb283 for Train set\n",
      "Processed mdb286 for Train set\n",
      "Processed mdb287 for Train set\n",
      "Processed mdb290 for Train set\n",
      "Processed mdb293 for Train set\n",
      "Processed mdb294 for Train set\n",
      "Processed mdb295 for Train set\n",
      "Processed mdb296 for Train set\n",
      "Processed mdb297 for Train set\n",
      "Processed mdb299 for Train set\n",
      "Processed mdb301 for Train set\n",
      "Processed mdb302 for Train set\n",
      "Processed mdb305 for Train set\n",
      "Processed mdb307 for Train set\n",
      "Processed mdb309 for Train set\n",
      "Processed mdb310 for Train set\n",
      "Processed mdb313 for Train set\n",
      "Processed mdb314 for Train set\n",
      "Processed mdb316 for Train set\n",
      "Processed mdb317 for Train set\n",
      "Processed mdb319 for Train set\n",
      "Processed mdb320 for Train set\n",
      "Processed mdb321 for Train set\n",
      "Processed mdb322 for Train set\n",
      "Processed mdb002 for Eval set\n",
      "Processed mdb012 for Eval set\n",
      "Processed mdb013 for Eval set\n",
      "Processed mdb016 for Eval set\n",
      "Processed mdb039 for Eval set\n",
      "Processed mdb071 for Eval set\n",
      "Processed mdb082 for Eval set\n",
      "Processed mdb102 for Eval set\n",
      "Processed mdb114 for Eval set\n",
      "Processed mdb116 for Eval set\n",
      "Processed mdb135 for Eval set\n",
      "Processed mdb147 for Eval set\n",
      "Processed mdb150 for Eval set\n",
      "Processed mdb157 for Eval set\n",
      "Processed mdb168 for Eval set\n",
      "Processed mdb187 for Eval set\n",
      "Processed mdb188 for Eval set\n",
      "Processed mdb189 for Eval set\n",
      "Processed mdb192 for Eval set\n",
      "Processed mdb199 for Eval set\n",
      "Processed mdb202 for Eval set\n",
      "Processed mdb217 for Eval set\n",
      "Processed mdb228 for Eval set\n",
      "Processed mdb231 for Eval set\n",
      "Processed mdb244 for Eval set\n",
      "Processed mdb269 for Eval set\n",
      "Processed mdb271 for Eval set\n",
      "Processed mdb272 for Eval set\n",
      "Processed mdb285 for Eval set\n",
      "Processed mdb289 for Eval set\n",
      "Processed mdb291 for Eval set\n",
      "Processed mdb298 for Eval set\n",
      "Processed mdb311 for Eval set\n",
      "Processed mdb004 for Test set\n",
      "Processed mdb006 for Test set\n",
      "Processed mdb008 for Test set\n",
      "Processed mdb010 for Test set\n",
      "Processed mdb017 for Test set\n",
      "Processed mdb026 for Test set\n",
      "Processed mdb034 for Test set\n",
      "Processed mdb043 for Test set\n",
      "Processed mdb046 for Test set\n",
      "Processed mdb047 for Test set\n",
      "Processed mdb056 for Test set\n",
      "Processed mdb058 for Test set\n",
      "Processed mdb061 for Test set\n",
      "Processed mdb064 for Test set\n",
      "Processed mdb076 for Test set\n",
      "Processed mdb077 for Test set\n",
      "Processed mdb078 for Test set\n",
      "Processed mdb079 for Test set\n",
      "Processed mdb083 for Test set\n",
      "Processed mdb085 for Test set\n",
      "Processed mdb091 for Test set\n",
      "Processed mdb094 for Test set\n",
      "Processed mdb095 for Test set\n",
      "Processed mdb105 for Test set\n",
      "Processed mdb109 for Test set\n",
      "Processed mdb115 for Test set\n",
      "Processed mdb117 for Test set\n",
      "Processed mdb119 for Test set\n",
      "Processed mdb120 for Test set\n",
      "Processed mdb127 for Test set\n",
      "Processed mdb133 for Test set\n",
      "Processed mdb144 for Test set\n",
      "Processed mdb159 for Test set\n",
      "Processed mdb165 for Test set\n",
      "Processed mdb173 for Test set\n",
      "Processed mdb174 for Test set\n",
      "Processed mdb180 for Test set\n",
      "Processed mdb186 for Test set\n",
      "Processed mdb196 for Test set\n",
      "Processed mdb197 for Test set\n",
      "Processed mdb198 for Test set\n",
      "Processed mdb205 for Test set\n",
      "Processed mdb209 for Test set\n",
      "Processed mdb210 for Test set\n",
      "Processed mdb226 for Test set\n",
      "Processed mdb227 for Test set\n",
      "Processed mdb230 for Test set\n",
      "Processed mdb245 for Test set\n",
      "Processed mdb249 for Test set\n",
      "Processed mdb256 for Test set\n",
      "Processed mdb257 for Test set\n",
      "Processed mdb262 for Test set\n",
      "Processed mdb270 for Test set\n",
      "Processed mdb282 for Test set\n",
      "Processed mdb284 for Test set\n",
      "Processed mdb288 for Test set\n",
      "Processed mdb292 for Test set\n",
      "Processed mdb300 for Test set\n",
      "Processed mdb303 for Test set\n",
      "Processed mdb304 for Test set\n",
      "Processed mdb306 for Test set\n",
      "Processed mdb308 for Test set\n",
      "Processed mdb312 for Test set\n",
      "Processed mdb315 for Test set\n",
      "Processed mdb318 for Test set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def np_CountUpContinuingOnes(b_arr):\n",
    "    left = np.arange(len(b_arr))\n",
    "    left[b_arr > 0] = 0\n",
    "    left = np.maximum.accumulate(left)\n",
    "    rev_arr = b_arr[::-1]\n",
    "    right = np.arange(len(rev_arr))\n",
    "    right[rev_arr > 0] = 0\n",
    "    right = np.maximum.accumulate(right)\n",
    "    right = len(rev_arr) - 1 - right[::-1]\n",
    "    return right - left - 1\n",
    "\n",
    "def ExtractBreast(img):\n",
    "    img_copy = img.copy()\n",
    "    img = np.where(img <= 20, 0, img)\n",
    "    height, _ = img.shape\n",
    "    y_a = height // 2 + int(height * 0.4)\n",
    "    y_b = height // 2 - int(height * 0.4)\n",
    "    b_arr = img[y_b:y_a].std(axis=0) != 0\n",
    "    continuing_ones = np_CountUpContinuingOnes(b_arr)\n",
    "    col_ind = np.where(continuing_ones == continuing_ones.max())[0]\n",
    "    img = img[:, col_ind]\n",
    "    _, width = img.shape\n",
    "    x_a = width // 2 + int(width * 0.4)\n",
    "    x_b = width // 2 - int(width * 0.4)\n",
    "    b_arr = img[:, x_b:x_a].std(axis=1) != 0\n",
    "    continuing_ones = np_CountUpContinuingOnes(b_arr)\n",
    "    row_ind = np.where(continuing_ones == continuing_ones.max())[0]\n",
    "    return img_copy[row_ind][:, col_ind]\n",
    "\n",
    "# 定义路径\n",
    "TXT_PATH = '/Volumes/图图/MIAS/archive/Info.txt'\n",
    "PGM_PATH = '/Volumes/图图/MIAS/archive/all-mias'\n",
    "OUTPUT_BASE_PATH = '../classification_data/MIAS-breast'\n",
    "SPLIT_CSV_PATH = '../classification_data/classification_split.csv'\n",
    "\n",
    "# 读取数据划分CSV文件\n",
    "split_df = pd.read_csv(SPLIT_CSV_PATH)\n",
    "split_df = split_df[split_df['dataset'] == 'MIAS-breast']\n",
    "\n",
    "# 读取TXT文件\n",
    "with open(TXT_PATH, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 跳过标题行\n",
    "lines = lines[1:]\n",
    "\n",
    "# 初始化一个字典，用于合并相同的 refnum 信息\n",
    "merged_data = {}\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    refnum = parts[0]\n",
    "    bg = str(parts[1]).replace(' ', '')\n",
    "    cls = str(parts[2]).replace(' ', '')\n",
    "    severity = str(parts[3]).replace(' ', '') if len(parts) > 3 else 'N'\n",
    "    if refnum in merged_data:\n",
    "        merged_data[refnum]['class'].add(cls)\n",
    "        if severity == 'M':\n",
    "            merged_data[refnum]['severity'] = 'M'\n",
    "    else:\n",
    "        merged_data[refnum] = {\n",
    "            'background': bg,\n",
    "            'class': {cls},\n",
    "            'severity': severity\n",
    "        }\n",
    "\n",
    "# 处理并保存数据\n",
    "for refnum, info in merged_data.items():\n",
    "    # 查找对应的data_split\n",
    "    split_info = split_df[split_df['data_name'] == refnum]\n",
    "    if split_info.empty:\n",
    "        print(f\"No split info found for {refnum}\")\n",
    "        continue\n",
    "    \n",
    "    data_split = split_info['data_split'].values[0]\n",
    "    \n",
    "    # 读取 PGM 文件\n",
    "    pgm_file = os.path.join(PGM_PATH, refnum + '.pgm')\n",
    "    if os.path.exists(pgm_file):\n",
    "        img = cv2.imread(pgm_file, cv2.IMREAD_GRAYSCALE)\n",
    "        img = ExtractBreast(img)\n",
    "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        # 创建输出目录\n",
    "        output_dir = os.path.join(OUTPUT_BASE_PATH, data_split, refnum)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 保存图像为 JPEG 格式\n",
    "        jpg_output_path = os.path.join(output_dir, 'img.jpg')\n",
    "        cv2.imwrite(jpg_output_path, img)\n",
    "\n",
    "        # 保存标签为 NumPy 文件字典\n",
    "        info_dict = {\n",
    "            'Composition': info['background']\n",
    "        }\n",
    "        npy_path = os.path.join(output_dir, 'info_dict.npy')\n",
    "        np.save(npy_path, info_dict)\n",
    "\n",
    "        print(f\"Processed {refnum} for {data_split} set\")\n",
    "    else:\n",
    "        print(f\"PGM file for {refnum} not found.\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cropped-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mdb219 for Train set\n",
      "Processed mdb013 for Train set\n",
      "Processed mdb170 for Train set\n",
      "Processed mdb256 for Train set\n",
      "Processed mdb097 for Train set\n",
      "Processed mdb178 for Train set\n",
      "Processed mdb091 for Train set\n",
      "Processed mdb105 for Train set\n",
      "Processed mdb107 for Train set\n",
      "Processed mdb083 for Train set\n",
      "Processed mdb248 for Train set\n",
      "Processed mdb214 for Train set\n",
      "Processed mdb271 for Train set\n",
      "Processed mdb102 for Train set\n",
      "Processed mdb063 for Train set\n",
      "Processed mdb241 for Train set\n",
      "Processed mdb015 for Train set\n",
      "Processed mdb267 for Train set\n",
      "Processed mdb152 for Train set\n",
      "Processed mdb030 for Train set\n",
      "Processed mdb090 for Train set\n",
      "Processed mdb252 for Train set\n",
      "Processed mdb239 for Train set\n",
      "Processed mdb095 for Train set\n",
      "Processed mdb274 for Train set\n",
      "Processed mdb080 for Train set\n",
      "Processed mdb204 for Train set\n",
      "Processed mdb081 for Train set\n",
      "Processed mdb264 for Train set\n",
      "Processed mdb058 for Train set\n",
      "Processed mdb134 for Train set\n",
      "Processed mdb005 for Train set\n",
      "Processed mdb142 for Train set\n",
      "Processed mdb171 for Train set\n",
      "Processed mdb191 for Train set\n",
      "Processed mdb121 for Train set\n",
      "Processed mdb188 for Train set\n",
      "Processed mdb184 for Train set\n",
      "Processed mdb249 for Train set\n",
      "Processed mdb115 for Train set\n",
      "Processed mdb211 for Train set\n",
      "Processed mdb179 for Train set\n",
      "Processed mdb141 for Train set\n",
      "Processed mdb072 for Train set\n",
      "Processed mdb195 for Train set\n",
      "Processed mdb167 for Train set\n",
      "Processed mdb244 for Train set\n",
      "Processed mdb132 for Train set\n",
      "Processed mdb005 for Train set\n",
      "Processed mdb226 for Train set\n",
      "Processed mdb158 for Train set\n",
      "Processed mdb226 for Train set\n",
      "Processed mdb198 for Train set\n",
      "Processed mdb160 for Train set\n",
      "Processed mdb144 for Train set\n",
      "Processed mdb145 for Train set\n",
      "Processed mdb028 for Train set\n",
      "Processed mdb206 for Train set\n",
      "Processed mdb199 for Train set\n",
      "Processed mdb165 for Train set\n",
      "Processed mdb290 for Train set\n",
      "Processed mdb127 for Train set\n",
      "Processed mdb190 for Train set\n",
      "Processed mdb312 for Train set\n",
      "Processed mdb240 for Train set\n",
      "Processed mdb239 for Train set\n",
      "Processed mdb227 for Train set\n",
      "Processed mdb110 for Train set\n",
      "Processed mdb025 for Train set\n",
      "Processed mdb144 for Train set\n",
      "Processed mdb012 for Train set\n",
      "Processed mdb032 for Train set\n",
      "Processed mdb181 for Train set\n",
      "Processed mdb075 for Train set\n",
      "Processed mdb002 for Train set\n",
      "Processed mdb315 for Train set\n",
      "Processed mdb253 for Train set\n",
      "Processed mdb213 for Train set\n",
      "Processed mdb125 for Train set\n",
      "Processed mdb117 for Train set\n",
      "Processed mdb019 for Train set\n",
      "Processed mdb226 for Train set\n",
      "Processed mdb150 for Train set\n",
      "Processed mdb163 for Eval set\n",
      "Processed mdb202 for Eval set\n",
      "Processed mdb099 for Eval set\n",
      "Processed mdb249 for Eval set\n",
      "Processed mdb130 for Eval set\n",
      "Processed mdb111 for Eval set\n",
      "Processed mdb017 for Eval set\n",
      "Processed mdb212 for Eval set\n",
      "Processed mdb209 for Eval set\n",
      "Processed mdb120 for Eval set\n",
      "Processed mdb218 for Eval set\n",
      "Processed mdb207 for Eval set\n",
      "Processed mdb155 for Test set\n",
      "Processed mdb223 for Test set\n",
      "Processed mdb010 for Test set\n",
      "Processed mdb132 for Test set\n",
      "Processed mdb092 for Test set\n",
      "Processed mdb314 for Test set\n",
      "Processed mdb186 for Test set\n",
      "Processed mdb021 for Test set\n",
      "Processed mdb223 for Test set\n",
      "Processed mdb193 for Test set\n",
      "Processed mdb069 for Test set\n",
      "Processed mdb270 for Test set\n",
      "Processed mdb023 for Test set\n",
      "Processed mdb236 for Test set\n",
      "Processed mdb126 for Test set\n",
      "Processed mdb104 for Test set\n",
      "Processed mdb222 for Test set\n",
      "Processed mdb001 for Test set\n",
      "Processed mdb238 for Test set\n",
      "Processed mdb265 for Test set\n",
      "Processed mdb175 for Test set\n",
      "Processed mdb124 for Test set\n",
      "Processed mdb231 for Test set\n",
      "Processed mdb148 for Test set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def crop_and_save(img, x, y, radius, output_path):\n",
    "    h, w = img.shape\n",
    "    x = int(x)\n",
    "    y = w - int(y)\n",
    "    radius = int(radius)\n",
    "    x1 = max(x - radius, 0)\n",
    "    y1 = max(y - radius, 0)\n",
    "    x2 = min(x + radius, img.shape[1])\n",
    "    y2 = min(y + radius, img.shape[0])\n",
    "\n",
    "    cropped_img = img[y1:y2, x1:x2]\n",
    "    cropped_img = cv2.normalize(cropped_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    cv2.imwrite(output_path, cropped_img)\n",
    "\n",
    "\n",
    "TXT_PATH = '/Volumes/图图/MIAS/archive/Info.txt'\n",
    "PGM_PATH = '/Volumes/图图/MIAS/archive/all-mias'\n",
    "OUTPUT_BASE_PATH = '../classification_data/MIAS-finding'\n",
    "SPLIT_CSV_PATH = '../classification_data/classification_split.csv'\n",
    "\n",
    "\n",
    "split_df = pd.read_csv(SPLIT_CSV_PATH)\n",
    "split_df = split_df[split_df['dataset'] == 'MIAS-finding']\n",
    "\n",
    "\n",
    "with open(TXT_PATH, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "lines = lines[1:]\n",
    "lines = [line for line in lines if len(line.split()) > 4]\n",
    "\n",
    "\n",
    "refnum_counter = {}\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    refnum = parts[0]\n",
    "    cls = str(parts[2]).replace(' ', '')\n",
    "    severity = str(parts[3]).replace(' ', '')\n",
    "    x, y, radius = parts[4], parts[5], parts[6]\n",
    "\n",
    "\n",
    "    if refnum in refnum_counter:\n",
    "        refnum_counter[refnum] += 1\n",
    "    else:\n",
    "        refnum_counter[refnum] = 1\n",
    "    refnum_with_suffix = f\"{refnum}_{refnum_counter[refnum]}\"\n",
    "\n",
    "\n",
    "    split_info = split_df[split_df['data_name'] == refnum_with_suffix]\n",
    "    if split_info.empty:\n",
    "        print(f\"No split info found for {refnum_with_suffix}\")\n",
    "        continue\n",
    "    \n",
    "    data_split = split_info['data_split'].values[0]\n",
    "\n",
    "\n",
    "    pgm_file = os.path.join(PGM_PATH, refnum + '.pgm')\n",
    "    if os.path.exists(pgm_file):\n",
    "        img = cv2.imread(pgm_file, cv2.IMREAD_GRAYSCALE)\n",
    "        output_dir = os.path.join(OUTPUT_BASE_PATH, data_split, refnum_with_suffix)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        jpg_output_path = os.path.join(output_dir, 'img.jpg')\n",
    "        crop_and_save(img, x, y, radius, jpg_output_path)\n",
    "\n",
    "\n",
    "        info_dict = {\n",
    "            'Finding': [cls],\n",
    "            'Pathology': severity\n",
    "        }\n",
    "        \n",
    "\n",
    "        updated_categories = []\n",
    "        for category in info_dict['Finding']:\n",
    "            if category == 'CALC':\n",
    "                updated_categories.append('Calcification')\n",
    "            elif category == 'CIRC':\n",
    "                updated_categories.append('Circumscribed masses')\n",
    "            elif category == 'SPIC':\n",
    "                updated_categories.append('Spiculated masses')\n",
    "            elif category == 'ARCH':\n",
    "                updated_categories.append('Architectural distortion')\n",
    "            elif category == 'ASYM':\n",
    "                updated_categories.append('Asymmetry')\n",
    "            elif category == 'MISC':\n",
    "                updated_categories.append('Miscellaneous')\n",
    "        if updated_categories:\n",
    "            info_dict['Finding'] = updated_categories\n",
    "        \n",
    "\n",
    "        if 'Pathology' in info_dict:\n",
    "            if info_dict['Pathology'] == 'B':\n",
    "                info_dict['Pathology'] = 'Benign'\n",
    "            elif info_dict['Pathology'] == 'M':\n",
    "                info_dict['Pathology'] = 'Malignant'\n",
    "        \n",
    "        npy_path = os.path.join(output_dir, 'info_dict.npy')\n",
    "        np.save(npy_path, info_dict)\n",
    "\n",
    "        print(f\"Processed {refnum_with_suffix} for {data_split} set\")\n",
    "    else:\n",
    "        print(f\"PGM file for {refnum} not found.\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
